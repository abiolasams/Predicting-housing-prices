# -*- coding: utf-8 -*-
"""Predicting house prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dKk1oNDyHu-klupCNj2cEGdRuy28plRn

importing required libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

color = sns.color_palette()
sns.set_style('darkgrid')

from scipy import stats
from scipy.stats import norm, skew #for some statistics

## loading the training and test data from kaggle

test_df=pd.read_csv('test.csv')
train_df=pd.read_csv('train.csv')

train_df.head()

#setting the target variable as the sale price column in the train_df dataframe
y_train=train_df['SalePrice']

test_df.head()

#Save the 'Id' column
train_ID = train_df['Id']
test_ID = test_df['Id']

"""#### Data Processing

analyzing the data found in the saleprice column
"""

sns.distplot(train_df['SalePrice'] , fit=norm)

"""since the target variable is skewed to the left we are going to log transform the data to achieve a normal distribution"""

train_df['log_saleprice'] = np.log(train_df['SalePrice'])

#Check the new distribution to see the normalized data
sns.distplot(train_df['log_saleprice'] , fit=norm);

"""### Feature Engineering

joining the test and train data where the saleprice column has been dropped to make it easier to work with for cleaning the data
"""

ntrain = train_df.shape[0]
ntest = test_df.shape[0]

y_train = train_df.log_saleprice.values
all_data = pd.concat((train_df, test_df)).reset_index(drop=True)
all_data.drop(['SalePrice','log_saleprice'], axis=1, inplace=True)
print("all_data size is : {}".format(all_data.shape))

"""#### Data Cleaning"""

#assessing all the columns in the data that have missing data 
forever=round(all_data.isnull().mean()*100,2)

forever = forever[forever > 0]

forever.sort_values(ascending=False)

f, ax = plt.subplots(figsize=(15, 12))
plt.xticks(rotation='90')
sns.barplot(x=forever.index, y=forever)
plt.xlabel('Features', fontsize=15)
plt.ylabel('missing values', fontsize=15)
plt.title('missing data by feature', fontsize=15)

"""Decided to drop all the columns with missing data"""

all_data=all_data.dropna(axis='columns')

all_data.head()

"""correlation matrix """

# Create correlation matrix
corr_matrix = all_data.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find features with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]

# Drop features that are greater than 0.95
all_data.drop(to_drop, axis=1, inplace=True)

fig = plt.figure(figsize= (30,30))
sns.heatmap(corr_matrix, annot=True, vmax=1, cmap='viridis', square=False)
#plt.savefig('correlationplot.png')

"""Feature Engineering """

## Why label encoding over one hot encoding 
## The number of categories is quite large as one-hot encoding can lead
## to high memory consumption

from sklearn.preprocessing import LabelEncoder

# creating instance of labelencoder
labelencoder = LabelEncoder()

# Assigning numerical values and storing in another column for training data 
all_data['Street_1'] = labelencoder.fit_transform(all_data['Street'])
all_data['LotShape_1'] = labelencoder.fit_transform(all_data['LotShape'])
all_data['LandContour_1'] = labelencoder.fit_transform(all_data['LandContour'])
all_data['LotConfig_1'] = labelencoder.fit_transform(all_data['LotConfig'])
all_data['LandSlope_1'] = labelencoder.fit_transform(all_data['LandSlope'])
all_data['Neighborhood_1'] = labelencoder.fit_transform(all_data['Neighborhood'])
all_data['Condition1_1'] = labelencoder.fit_transform(all_data['Condition1'])
all_data['Condition2_1'] = labelencoder.fit_transform(all_data['Condition2'])

"""selecting only the columns that have integers"""

all_data=all_data.select_dtypes(include=['int64']).copy()

all_data.head()

"""# Modelling"""

from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn import preprocessing
from sklearn.model_selection import cross_validate

from xgboost import XGBRegressor
from sklearn.model_selection import KFold, cross_val_score

"""obtaining the train and test set again"""

train = all_data[:ntrain]
test = all_data[ntrain:]

"""Using cross validation """

# Setup cross validation folds
kf = KFold(n_splits=12, random_state=42, shuffle=True)

# Define error metrics
def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y_train, y_pred))

def cv_rmse(model, X=train):
    rmse = np.sqrt(-cross_val_score(model, train, y_train, scoring="neg_mean_squared_error", cv=kf))
    return (rmse)

"""Using random forest regression and XG Boost"""

# Random Forest Regressor
rf = RandomForestRegressor(n_estimators=1200,
                          max_depth=15,
                          min_samples_split=5,
                          min_samples_leaf=5,
                          max_features=None,
                          oob_score=True,
                          random_state=42)

# XGBoost Regressor
xgboost = XGBRegressor(learning_rate=0.01,
                       n_estimators=6000,
                       max_depth=4,
                       min_child_weight=0,
                       gamma=0.6,
                       subsample=0.7,
                       colsample_bytree=0.7,
                       objective='reg:linear',
                       nthread=-1,
                       scale_pos_weight=1,
                       seed=27,
                       reg_alpha=0.00006,
                       random_state=42)

"""Training the models """

scores={}
score = cv_rmse(rf)
print("rf: {:.4f} ({:.4f})".format(score.mean(), score.std()))
scores['rf'] = (score.mean(), score.std())

score = cv_rmse(xgboost)
print("xgboost: {:.4f} ({:.4f})".format(score.mean(), score.std()))
scores['xgb'] = (score.mean(), score.std())

"""Fitting the models """

print('RandomForest')
rf_model_full_data = rf.fit(train, y_train)

print('xgboost')
xgb_model_full_data = xgboost.fit(train, y_train)

"""Utilizing help from Lavanya Shukla's "How I made top 0.3% on a kaggle competition" we carry out blended predictions"""

# Blend models in order to make the final predictions more robust to overfitting
def blended_predictions(X):
    return (((0.1 * xgb_model_full_data.predict(X)) + \
            (0.05 * rf_model_full_data.predict(X))
            ))

# Get final precitions from the blended model
blended_score = rmsle(y_train, blended_predictions(train))
scores['blended'] = (blended_score, 0)
print('RMSLE score on train data:')
print(blended_score)

"""Best performing model """

# Plot the predictions for each model
sns.set_style("white")
fig = plt.figure(figsize=(24, 12))

ax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])
for i, score in enumerate(scores.values()):
    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')

plt.ylabel('Score (RMSE)', size=20, labelpad=12.5)
plt.xlabel('Model', size=20, labelpad=12.5)
plt.tick_params(axis='x', labelsize=13.5)
plt.tick_params(axis='y', labelsize=12.5)

plt.title('Scores of Models', size=20)

plt.show()

## Final Conclusion