# -*- coding: utf-8 -*-
"""Predicting house prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dKk1oNDyHu-klupCNj2cEGdRuy28plRn

importing required libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

color = sns.color_palette()
sns.set_style('darkgrid')

from scipy import stats
from scipy.stats import norm, skew #for some statistics

## loading the training and test data from kaggle

test_df=pd.read_csv('test.csv')
train_df=pd.read_csv('train.csv')

train_df.head()

#setting the target variable as the sale price column in the train_df dataframe
y_train=train_df['SalePrice']

test_df.head()

#Save the 'Id' column
train_ID = train_df['Id']
test_ID = test_df['Id']

"""#### Data Processing

analyzing the data found in the saleprice column
"""

sns.distplot(train_df['SalePrice'] , fit=norm)

"""since the target variable is skewed to the left we are going to log transform the data to achieve a normal distribution"""

train_df['log_saleprice'] = np.log(train_df['SalePrice'])

#Check the new distribution to see the normalized data
sns.distplot(train_df['log_saleprice'] , fit=norm);

"""### Feature Engineering

joining the test and train data where the saleprice column has been dropped to make it easier to work with for cleaning the data
"""

ntrain = train_df.shape[0]
ntest = test_df.shape[0]

y_train = train_df.log_saleprice.values
all_data = pd.concat((train_df, test_df)).reset_index(drop=True)
all_data.drop(['SalePrice','log_saleprice'], axis=1, inplace=True)
print("all_data size is : {}".format(all_data.shape))

"""#### Data Cleaning"""

#assessing all the columns in the data that have missing data 
number_of_null=all_data.isnull().sum()*100.0/all_data.shape[0]

number_of_null=number_of_null != 0

columns_null=number_of_null.sort_values(ascending=False)

f, ax = plt.subplots(figsize=(15, 12))
plt.xticks(rotation='90')
sns.barplot(x=columns_null.index, y=columns_null)
plt.xlabel('Features', fontsize=15)
plt.ylabel('missing values', fontsize=15)
plt.title('missing data by feature', fontsize=15)

"""Decided to drop all the columns with missing data"""

all_data=all_data.dropna(axis='columns')

all_data.head()

"""correlation matrix """

# Create correlation matrix
corr_matrix = all_data.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find features with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]

# Drop features that are greater than 0.95
all_data.drop(to_drop, axis=1, inplace=True)

fig = plt.figure(figsize= (30,30))
sns.heatmap(corr_matrix, annot=True, vmax=1, cmap='viridis', square=False)
#plt.savefig('correlationplot.png')

"""Feature Engineering """

## Why label encoding over one hot encoding 
## The number of categories is quite large as one-hot encoding can lead
## to high memory consumption

from sklearn.preprocessing import LabelEncoder

# creating instance of labelencoder
labelencoder = LabelEncoder()

# Assigning numerical values and storing in another column for training data 
all_data['Street_1'] = labelencoder.fit_transform(all_data['Street'])
all_data['LotShape_1'] = labelencoder.fit_transform(all_data['LotShape'])
all_data['LandContour_1'] = labelencoder.fit_transform(all_data['LandContour'])
all_data['LotConfig_1'] = labelencoder.fit_transform(all_data['LotConfig'])
all_data['LandSlope_1'] = labelencoder.fit_transform(all_data['LandSlope'])
all_data['Neighborhood_1'] = labelencoder.fit_transform(all_data['Neighborhood'])
all_data['Condition1_1'] = labelencoder.fit_transform(all_data['Condition1'])
all_data['Condition2_1'] = labelencoder.fit_transform(all_data['Condition2'])

"""selecting only the columns that have integers"""

all_data=all_data.select_dtypes(include=['int64']).copy()

all_data.head()

"""# Modelling

obtaining the train and test set again
"""

train = all_data[:ntrain]
test = all_data[ntrain:]

"""Random Forest Regression"""

from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn import preprocessing
from sklearn.model_selection import cross_validate

#fitting the initial random forest regressor to the training data
random = RandomForestRegressor()
random.fit(train, test)

## Gradient Boosting

## Final Conclusion